{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "nltk.download('all',quiet=True)\n",
    "from PIL import Image\n",
    "\n",
    "#Model libraries\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mounting drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning variable\n",
    "df_orignal=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Capstone Projects /Twitter Sentiment Analysis/Coronavirus Tweets.csv', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying data to preserve orignal file\n",
    "df1=df_orignal.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking Head\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking info\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking Columns\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For sentiment analysis we only want tweet and sentiment Features\n",
    "df=df1[['OriginalTweet','Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stastastical analysis of dataset\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking Unique values\n",
    "df.Sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking Shape of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check duplicate entries\n",
    "len(df[df.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"OriginalTweet\"] = df[\"OriginalTweet\"].str.lower()\n",
    "df['OriginalTweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OriginalTweet'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OriginalTweet'] = df['OriginalTweet'].str.replace('http\\S+|www.\\S+', '', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_tweets\"] = df['OriginalTweet'].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweets'] = df['clean_tweets'].str.replace(\"[^a-zA-Z#//]\",\" \")\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweets'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Stop-words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove stopwords and tokenize\n",
    "def remove_stopwords(text):\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweets']= df['clean_tweets'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.clean_tweets[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for stemming\n",
    "def stemming(text):    \n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return (\" \".join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stemmed'] = df['clean_tweets'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result\n",
    "df.stemmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "df['lemmed'] = df['clean_tweets'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_count = df['Sentiment'].value_counts().reset_index()\n",
    "sentiment_count.columns = ['Sentiment','count']\n",
    "sentiment_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "ax = sns.barplot(x=\"Sentiment\", y='count', data=sentiment_count)\n",
    "ax.set_title(\"Proportion of Sentiment\", fontsize=20)\n",
    "ax.set_xlabel(\"Sentiment\", fontsize=20)\n",
    "ax.set_ylabel('count', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing values\n",
    "replace_values = {\"Sentiment\":{'Extremely Negative':'Negative', 'Extremely Positive':'Positive'}}\n",
    "df = df.replace(replace_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_count1 = df['Sentiment'].value_counts().reset_index()\n",
    "sentiment_count1.columns = ['Sentiment','count']\n",
    "sentiment_count1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the piechart for Sentiments distribution\n",
    "sentiment_count1 = df['Sentiment'].value_counts().to_list()\n",
    "labels=['Positive','Negative','Netural']\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.pie(x=sentiment_count1,explode=[0.04,0.04,0.1],shadow= True,labels=labels,autopct=\"%.2f%%\",radius=1.1)\n",
    "plt.title(\"Proportion Of Sentiments\", fontsize=20)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['temp_list'] = df['clean_tweets'].apply(lambda x:str(x).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "top = Counter([item for sublist in df['temp_list'] for item in sublist])\n",
    "temp = pd.DataFrame(top.most_common(20))\n",
    "temp.columns = ['Common_words','count']\n",
    "temp.style.background_gradient(cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating the sentiments for word cloud \n",
    "neutral = pd.DataFrame(df[['stemmed','lemmed']] [df['Sentiment'] == 'Neutral'])\n",
    "positive = pd.DataFrame(df[['stemmed','lemmed']]  [df['Sentiment'] == 'Positive'])\n",
    "negative = pd.DataFrame(df[['stemmed','lemmed']]  [df['Sentiment'] == 'Negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array(Image.open(\"/content/drive/MyDrive/Colab Notebooks/Capstone Projects /Twitter Sentiment Analysis/toppng.com-transparent-background-twitter-logo-943x800.png\"))\n",
    "\n",
    "\n",
    "wc = WordCloud(background_color='white',mask = mask,contour_width=1,contour_color='steelblue')\n",
    "wc.generate(str(neutral['lemmed']))\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(wc,interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating word cloud for positive sentiments\n",
    "wc.generate(str(positive['lemmed']))\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating word cloud for negative sentiments\n",
    "wc.generate(str(negative['lemmed']))\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assigning dependent and independent features\n",
    "\n",
    "X= df['lemmed']\n",
    "y=df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Train test split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,stratify=y,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking shape of splitted data\n",
    "print(X_train.shape)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking splitted data\n",
    "print(X_train.head())\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "cv=CountVectorizer(binary=False,max_df=1.0,min_df=5,ngram_range=(1,2))\n",
    "cv_X_train=cv.fit_transform(X_train.astype(str).str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tv=TfidfVectorizer(use_idf=True,max_df=1.0,min_df=5,ngram_range=(1,2),sublinear_tf=True)\n",
    "tv_X_train=tv.fit_transform(X_train.astype(str).str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_X_test=cv.transform(X_test.astype(str).str.strip())\n",
    "tv_X_test=tv.transform(X_test.astype(str).str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initalizing the model\n",
    "lr_cv = LogisticRegression()\n",
    "parameters = dict(penalty=['l1', 'l2'],C=[100, 10, 1.0, 0.1, 0.01])\n",
    "\n",
    "#Hyperparameter tuning by GridserchCV\n",
    "logreg_Gcv=GridSearchCV(lr_cv,parameters,cv=15)\n",
    "\n",
    "#fitting the data to model\n",
    "logreg_Gcv.fit(cv_X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted values\n",
    "pred_lr_cv = logreg_Gcv.predict(cv_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "accuracy_lr_cv = accuracy_score(y_test,pred_lr_cv)\n",
    "print(\"Accuracy :\",(accuracy_lr_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Performance metrics\n",
    "label=['neutral','positive','negative']\n",
    "print(classification_report(y_test,pred_lr_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Confussion matrix\n",
    "cf1= (confusion_matrix(y_test,pred_lr_cv))\n",
    "plt.figure(figsize=(8,5))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cf1, annot=True, fmt=\".0f\",ax = ax)\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels', fontsize=15)\n",
    "ax.set_ylabel('Actual labels', fontsize=15)\n",
    "ax.set_title('Confusion Matrix (Logistic Regression with CV)', fontsize=20)\n",
    "ax.xaxis.set_ticklabels(labels)\n",
    "ax.yaxis.set_ticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "dt_cv=DecisionTreeClassifier()\n",
    "\n",
    "#fitting the data to model\n",
    "dt_cv.fit(cv_X_train,y_train)\n",
    "\n",
    "#predicted values\n",
    "pred_dt_cv=dt_cv.predict(cv_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dt_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "cv_score_dt_cv= cross_val_score(dt_cv,cv_X_train,y_train, cv=5)\n",
    "print(\"Accuracy: {}\" .format(np.mean(cv_score_dt_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Performance metrics\n",
    "label=['Neutral','Positive','Negative']\n",
    "print(classification_report(y_test,pred_dt_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Confussion matrix\n",
    "cf2= (confusion_matrix(y_test,pred_dt_cv))\n",
    "plt.figure(figsize=(8,5))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cf2, annot=True, fmt=\".0f\",ax = ax)\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels', fontsize=15)\n",
    "ax.set_ylabel('Actual labels', fontsize=15)\n",
    "ax.set_title('Confusion Matrix (Decision tree with CV)', fontsize=20)\n",
    "ax.xaxis.set_ticklabels(labels)\n",
    "ax.yaxis.set_ticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "xgb_cv=XGBClassifier()\n",
    "\n",
    "#fitting the data to model\n",
    "xgb_cv.fit(cv_X_train,y_train)\n",
    "\n",
    "#predicted values\n",
    "pred_xgb_cv=xgb_cv.predict(cv_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "cv_score_xgb_cv= cross_val_score(xgb_cv,cv_X_train,y_train, cv=5)\n",
    "print(\"Accuracy: {}\" .format(np.mean(cv_score_xgb_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Performance metrics\n",
    "label=['Neutral','Positive','Negative']\n",
    "print(classification_report(y_test,pred_xgb_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Confussion matrix\n",
    "cf3= (confusion_matrix(y_test,pred_xgb_cv))\n",
    "plt.figure(figsize=(8,5))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cf3, annot=True, fmt=\".0f\",ax = ax)\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels', fontsize=15)\n",
    "ax.set_ylabel('Actual labels', fontsize=15)\n",
    "ax.set_title('Confusion Matrix (XG-Boost with CV)', fontsize=20)\n",
    "ax.xaxis.set_ticklabels(labels)\n",
    "ax.yaxis.set_ticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "knn = KNeighborsClassifier()\n",
    "param = {'n_neighbors': [1,2,3,4,5,6,7,8]}\n",
    "knn_cv = GridSearchCV(estimator=knn,param_grid=param)\n",
    "\n",
    "#fitting the data to model\n",
    "knn_cv.fit(cv_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted values\n",
    "pred_knn_cv = knn_cv.predict(cv_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_knn_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy_KNN = accuracy_score(y_test,pred_knn_cv)\n",
    "print(\"Accuracy :\",(accuracy_KNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Performance metrics\n",
    "label=['Neutral','Positive','Negative']\n",
    "print(classification_report(y_test,pred_knn_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Confussion matrix\n",
    "cf4= (confusion_matrix(y_test,pred_knn_cv))\n",
    "plt.figure(figsize=(8,5))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cf4, annot=True, fmt=\".0f\",ax = ax)\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels', fontsize=15)\n",
    "ax.set_ylabel('Actual labels', fontsize=15)\n",
    "ax.set_title('Confusion Matrix (KNN with CV)', fontsize=20)\n",
    "ax.xaxis.set_ticklabels(labels)\n",
    "ax.yaxis.set_ticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "svm_cv = SVC()\n",
    "\n",
    "#fitting the data to model\n",
    "svm_cv.fit(cv_X_train,y_train)\n",
    "\n",
    "#prediction\n",
    "pred_svm_cv = svm_cv.predict(cv_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy_svc = accuracy_score(y_test,pred_svm_cv)\n",
    "print(\"Accuracy :\",(accuracy_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Performance metrics\n",
    "label=['Neutral','Positive','Negative']\n",
    "print(classification_report(y_test,pred_svm_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Confussion matrix\n",
    "cf5= (confusion_matrix(y_test,pred_svm_cv))\n",
    "plt.figure(figsize=(8,5))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cf5, annot=True, fmt=\".0f\",ax = ax)\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels', fontsize=15)\n",
    "ax.set_ylabel('Actual labels', fontsize=15)\n",
    "ax.set_title('Confusion Matrix (SVM with CV)', fontsize=20)\n",
    "ax.xaxis.set_ticklabels(labels)\n",
    "ax.yaxis.set_ticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "lr_tv=LogisticRegression()\n",
    "parameters = dict(penalty=['l1', 'l2'],C=[100, 10, 1.0, 0.1, 0.01])\n",
    "\n",
    "#Hyperparameter tuning by GridserchCV\n",
    "lr_tv_Gcv=GridSearchCV(lr_tv,parameters,cv=5)\n",
    "\n",
    "#fitting the data to model\n",
    "lr_tv_Gcv.fit(tv_X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted values\n",
    "pred_lr_tv_Gcv = lr_tv_Gcv.predict(tv_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr_tv_Gcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "accuracy_lr_Gcv = accuracy_score(y_test,pred_lr_tv_Gcv)\n",
    "print(\"Accuracy :\",(accuracy_lr_Gcv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Performance metrics\n",
    "label=['Neutral','Positive','Negative']\n",
    "print(classification_report(y_test,pred_lr_tv_Gcv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Confussion matrix\n",
    "cf1a= (confusion_matrix(y_test,pred_lr_tv_Gcv))\n",
    "plt.figure(figsize=(8,5))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cf1a, annot=True, fmt=\".0f\",ax = ax)\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels', fontsize=15)\n",
    "ax.set_ylabel('Actual labels', fontsize=15)\n",
    "ax.set_title('Confusion Matrix (Logistic Regg with TF/IDF)', fontsize=20)\n",
    "ax.xaxis.set_ticklabels(labels)\n",
    "ax.yaxis.set_ticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "dt_tv=DecisionTreeClassifier()\n",
    "\n",
    "#fitting the data to model\n",
    "dt_tv.fit(tv_X_train,y_train)\n",
    "\n",
    "#prediction\n",
    "pred_dt_tv=dt_tv.predict(tv_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dt_tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "cv_score_dt_tv= cross_val_score(dt_tv,tv_X_train,y_train, cv=5)\n",
    "print(\"Accuracy: {}\" .format(np.mean(cv_score_dt_tv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Performance metrics\n",
    "label=['Neutral','Positive','Negative']\n",
    "print(classification_report(y_test,pred_dt_tv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Confussion matrix\n",
    "cf2a= (confusion_matrix(y_test,pred_dt_tv))\n",
    "plt.figure(figsize=(8,5))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cf2a, annot=True, fmt=\".0f\",ax = ax)\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels', fontsize=15)\n",
    "ax.set_ylabel('Actual labels', fontsize=15)\n",
    "ax.set_title('Confusion Matrix (Decision Tree with TF/IDF)', fontsize=20)\n",
    "ax.xaxis.set_ticklabels(labels)\n",
    "ax.yaxis.set_ticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "xgb_tv=DecisionTreeClassifier()\n",
    "\n",
    "#fitting the data to model\n",
    "xgb_tv.fit(tv_X_train,y_train)\n",
    "\n",
    "#prediction\n",
    "pred_xgb_tv=xgb_tv.predict(tv_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb_tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "cv_score_xgb_tv= cross_val_score(xgb_tv,tv_X_train,y_train, cv=5)\n",
    "print(\"Accuracy: {}\" .format(np.mean(cv_score_xgb_tv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Performance metrics\n",
    "label=['Neutral','Positive','Negative']\n",
    "print(classification_report(y_test,pred_xgb_tv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Confussion matrix\n",
    "cf3a= (confusion_matrix(y_test,pred_xgb_tv))\n",
    "plt.figure(figsize=(8,5))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cf3a, annot=True, fmt=\".0f\",ax = ax)\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels', fontsize=15)\n",
    "ax.set_ylabel('Actual labels', fontsize=15)\n",
    "ax.set_title('Confusion Matrix (XG Boost with TF/IDF)', fontsize=20)\n",
    "ax.xaxis.set_ticklabels(labels)\n",
    "ax.yaxis.set_ticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "knn = KNeighborsClassifier()\n",
    "param = {'n_neighbors': [1,2,3,4,5,6,7,8]}\n",
    "knn_tv = GridSearchCV(estimator=knn,param_grid=param)\n",
    "\n",
    "#fitting the data to model\n",
    "knn_tv.fit(tv_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted values\n",
    "pred_knn_tv = knn_cv.predict(tv_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_knn_tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy_KNN_tv = accuracy_score(y_test,pred_knn_tv)\n",
    "print(\"Accuracy :\",(accuracy_KNN_tv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Performance metrics\n",
    "label=['Neutral','Positive','Negative']\n",
    "print(classification_report(y_test,pred_knn_tv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Confussion matrix\n",
    "cf4a= (confusion_matrix(y_test,pred_xgb_tv))\n",
    "plt.figure(figsize=(8,5))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cf4a, annot=True, fmt=\".0f\",ax = ax)\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels', fontsize=15)\n",
    "ax.set_ylabel('Actual labels', fontsize=15)\n",
    "ax.set_title('Confusion Matrix (KNN TF/IDF with GridsearchCV)', fontsize=20)\n",
    "ax.xaxis.set_ticklabels(labels)\n",
    "ax.yaxis.set_ticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "svm_tv = SVC()\n",
    "\n",
    "#fitting the data to model\n",
    "svm_tv.fit(tv_X_train,y_train)\n",
    "\n",
    "#prediction\n",
    "pred_svm_tv = svm_tv.predict(tv_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm_tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy_svm_tv = accuracy_score(y_test,pred_svm_tv)\n",
    "print(\"Accuracy :\",(accuracy_svm_tv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Performance metrics\n",
    "label=['Neutral','Positive','Negative']\n",
    "print(classification_report(y_test,pred_svm_tv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Confussion matrix\n",
    "cf5a= (confusion_matrix(y_test,pred_svm_tv))\n",
    "plt.figure(figsize=(8,5))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cf5a, annot=True, fmt=\".0f\",ax = ax)\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels', fontsize=15)\n",
    "ax.set_ylabel('Actual labels', fontsize=15)\n",
    "ax.set_title('Confusion Matrix (KNN TF/IDF with GridsearchCV)', fontsize=20)\n",
    "ax.xaxis.set_ticklabels(labels)\n",
    "ax.yaxis.set_ticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's acurracy Score Comparision\n",
    "\n",
    "acurracy = {'Model':  ['Logistic Regression with GridserachCV', 'Decision Tree Classifier','XG-Boost Classifier','K-Nearest-Neighbours Classifier','Support-Vector-Machine Classifier'],\n",
    "        'Count Vector':  [accuracy_lr_cv,np.mean(cv_score_dt_cv),np.mean(cv_score_xgb_cv),accuracy_KNN,accuracy_svc],\n",
    "        'Tf/idf Vector': [accuracy_lr_Gcv,np.mean(cv_score_dt_tv),np.mean(cv_score_xgb_tv),accuracy_KNN_tv,accuracy_svm_tv]}\n",
    "\n",
    "cv_score_table= pd.DataFrame (acurracy, columns = ['Model','Count Vector','Tf/idf Vector'])\n",
    "\n",
    "cv_score_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
